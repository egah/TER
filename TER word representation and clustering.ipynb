{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word representation and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing is the technology used to aid computers to understand the human's language. Computational linguists do it by trying to find the meaning of words, the idea behind it.\n",
    "In natural language processing, words are represented as vectors of real numbers. The computation of these values is usually expensive but this representation allows the definition of mathematical concepts, distances for instance that can be used to find possible connections between words. \n",
    "However, the nature of the connection cannot be grasped by a scalar as it can have different shapes : synonyms, antonyms etc.\n",
    "Consequently, word representation methods are expected to resolve the computation and word-sense disambiguation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Word representation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Document-term matrix (DTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word representation method we use is the document-term matrix. This method first turns each document into a vector by counting how many times each word appear in the document, in other words, its frequency. This transformation is what we call a bag-of-word model because the order of the words does not matter in this representation. Then, this method produces a matrix $M$ with each $M_{i,j}$ describing the frequency of the j-th term in the i-th document. \n",
    "This representation allows some comparison between documents based on the dot product. Let V be the number of unique words in the corpus, $a_i$ and $b_i$ the i-th word's frequency for two distinct documents. Computing $\\sum_{i=1}^{T} a_ib_i$  gives the similarity between the two documents. The higher the result is, the more similar the two documents are. \n",
    "There are however major flaws with this method :\n",
    "- First, the dot product only counts the occurences of the exact same words. If we computed the dot product of the words \"cat\" and \"cats\" for example, we would get a 0, as if there were no relationship between these words.\n",
    "- Second, most common words which are usually prepositions, weigh more than rarer words like nouns even though in indo-european languages, such words carry less meaning.\n",
    "- Finally, for a vocabular of size V and a number of documents N, the worst case time complexity of the algorithm is $O$(__VN__), which is a quadratic time. However, in practice the terms will not be present in all the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Term Frequency - Inverse document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TF-IDF representation is another statistical approach that can correct some of the issues mentionned above. This method simply divides the term frequency in a document by the frequency of the term for all the documents. This way, each word is inversely proportional to the number of documents it appears in. Not only it normalizes the words weights but it also highlights rare words.\n",
    "In sum, TF-IDF is a way to give better weights to words according to their relevance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods discussed above are what we call localist methods. They characterize documents on one unit causing the \"meaning of the words is stored in only one place\" [1]. The inferences we can make are at document level but for a better analysis we need a numerical representation for each word. Such methods exists and they are called word embeddings methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third method we will study is a word embedding method called word2vec. It works with the combination of two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a general idea that you can get the meaning of a word by looking at its context. Based on this principle, the Skip-gram model aims to predict the context words around a center word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram in details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a center word $w_t$ the model defines the unique probability of a word appearing in its context within a certain window. It is a probability distribution. The window size is controlled by a parameter $m$ which works like a radius because the window size is for both sides of the center word. We would like to have the words which have the highest chances of appearing in the context of $w_t$ so we have to choose the word vector that maximize this probability distribution. The parameter of the model that we call $\\theta$ is exactly this words's vector. \n",
    "In order to find the best parameters $\\theta$ we use what we call a loss function $J = 1 - P(w_{-t}|w_t)$ with $w_{-t}$ being the words around $w_t$ which ables us to adjust the parameters $\\theta$ and find that maximum. It is done by minimizing the derivative of the loss function, which can be written as follows : $$J'(\\theta)=\\prod_{t=1}^{T} \\prod_{-m\\leq j \\leq m; j\\neq 0}\\operatorname{P}(w_{t+j}|w_t;\\theta) $$\n",
    "Then, we can use the log likelihood which facilitate the computation by changing products into sums without changing  the maximum that we will find.\n",
    "$$ J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} log\\operatorname{P}(w_{t+j}|w_t;\\theta) $$\n",
    "The $\\frac{1}{T}$ is used to \"normalize\" the result in order for it to stay in a valid range.\n",
    "Before doing that, since word vectors are not probabilities we have to transform them. The simplest way to get a probability distribution is by using the function $\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$. It is efficient because it will make any value positive and in the [0,1] range.\n",
    "Each word in this model have two representation, one as a context word and another as a center word because it is much easier for the optimization process rather than when vectors are tied to each other. \n",
    "As such, if we define $V_c$ the vector of the center word of indice c and $u_o$ the context word of indice o, we would formally use the softmax function as follows : $$ \\operatorname{P}(o|c) = \\frac{\\exp(u_o^{T}V_c)}{\\sum_{w=1}^W \\exp(u_w^{T})V_c} $$\n",
    "To sum up, if we reorder this process in the right order for a vocabulary of size V and vectors of dimension d, we first multiply the center word $w_t$ which is a one hot vector with the matrix $W$, representing the center words. This multiplication selects the column of the matrix which corresponds to $w_t$, we call it $V_c$. Then, we compute the dot product of $V_c$ with each context word representation $u_o$ stored in a matrix $O$. We feed the result $u_o^{T}V_c$ to the softmax function to turn it into a probability $ \\operatorname{P}(o|c) = \\text{Softmax}(u_o^{T}V_c)$. This gives us the probability for each word of appearing in the context of $w_t$. \n",
    "Finally, we have a truth vector that is a one hot vector showing which word should have the highest probability.  We use it to optimize the parameters by minimizing this loss i.e we find the best vector $\\theta$ with the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Bag Of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second algorithm is the continuous bag of word.\n",
    "CBOW does the complementary work, it predicts the target word from the context words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW in details "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word is encoded as a one hot vector, with a dimension of the size of the vocabulary. For a window size m, we try to predict the center word by using the m-1 words surrounding it. It is done with a neural network that takes the context words as input and tries to predict the center word. Then, we feed it to a training function to get a probability that is compared to the truth vector. We use the error to update the weights of the vector with the same process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the Word2vec model aims to be a scalable and fast word embedding model that can be run over billions of words and produce good word representation with usually a dimension of 300. It is very different and even opposed to the first methods as it is a distributional based representation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an exploratory data analysis technique used to identify clusters in a database. Our goal when we perform a clustering algorithm is to find subgroups in the data. The data points in the same subgroup have to be very similar. On the other hand, the data points in different subgroups have to be very different. Put another way, our goal is to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according to a similarity measure such as the euclidean distance. The decision of which similarity measure to use depend on the domain.\n",
    "\n",
    "Clustering analysis can be done based on featurmes where we try to find subgroups of samples based on features or based on samples where we try to find subgroups of features based on samples. We will cover here clustering based on features. \n",
    "Unlike supervised learning, clustering is considered an unsupervised learning method since we do not have the ground truth to compare the output of the clustering algorithm to the true labels to evaluate its performance. We only want to try to find, if possible, a particular structure in our database.\n",
    "In this paragraph, we will talk about Kmeans : one of the most used clustering algorithm due to its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does K-means works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Measure - Euclidean distance \n",
    "When we perform k-means in order to see if data points are similar, we need a similarity measure. There are various similarity measures according to the domain. Most commonly, we use the euclidean distance.\n",
    "Let denote by $X$ and $Z$ to sample pattern vectors, $X={\\left({x}_{1},{x}_{2},...,{x}_{n}\\right)}^{T}$ and $Z={\\left({z}_{1},{z}_{2},...,{z}_{n}\\right)}^{T}$. We define the distance between $X$ and $Z$ as:\n",
    "$$d=||X-Z||= {\\left(\\sum_{i=1}^{n}  {\\left({x}_{i}-{z}_{i}\\right)}^{2}\\right)}^{1/2}$$\n",
    "$D$ is the distance between $X$ and $Z$ in a n dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inertia\n",
    "Let denote by $ E= \\left({x}_{1},{x}_{2},...,{x}_{n}\\right) \\subset \\mathbb{R}^{n}$. For each $x{}_{i} \\in E $ ze associate a weight ${m}_{{x}_{i}} > 0$.\n",
    "For everey subset $A$ of $E$ we define the weight:\n",
    "$${m}_{A} = \\sum_{x \\in A} {m}_{x}$$\n",
    "The gravity center of A is:\n",
    "$${g}_{A}=\\frac{ \\sum_{x \\in A} {m}_{x} \\times x}{{m}_{A}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inertia of $E$ define by:\n",
    "$$I\\left(E\\right)=\\sum_{x \\in E} {m}_{x} \\times {d}^{2}\\left(x,{g}_{E}\\right)$$\n",
    "Let define by $P = {{A}_{1},{A}_{2},...,}{A}_{q}$ a partition of $E$. For each subset ${A}_{i}$, we consider ${g}_{{A}_{i}}$ its gravity center,${I\\left({A}_{i}\\right)}$ and ${m}_{{A}_{i}}$ his weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within cluster sum of squares\n",
    "The within-cluster sum of squares is a quantity that measures the variability of the observations within each cluster. A small within-cluster sum of squares means that the data points within the cluster are similar and a large within-cluster sum of squares means that observations within the cluster are fareaway from each other.\n",
    "The within cluster sum of squares is influenced by the number of observations. As the number of observations increases, the sum of squares becomes larger.\n",
    "We consider ${I}_{W}$ the within-cluster sum of squares.\n",
    "$${I}_{W}= \\sum_{i}{}I\\left({A}_{i}\\right)$$\n",
    "\n",
    "#### Within cluster sum of squares\n",
    "The within-cluster sum of squares is a quantity that measures the variability of the observations within each cluster. A small within-cluster sum of squares means that the data points within the cluster are similar and a large within-cluster sum of squares means that observations within the cluster are fareaway from each other.\n",
    "The within cluster sum of squares is influenced by the number of observations. As the number of observations increases, the sum of squares becomes larger.\n",
    "We consider ${I}_{W}$ the within-cluster sum of squares.\n",
    "$${I}_{W}= \\sum_{i}{}I\\left({A}_{i}\\right)$$\n",
    "\n",
    "#### Huygens theorem\n",
    "$$Total \\space Sum \\space Squares = Between \\space Cluesters \\space Sum \\space Squares + Within \\space Cluesters \\space Sum \\space Squares$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of k-means is to find the $K$ initial centroids, where $K$ is the number of clusters we want to obtain. Each point is then assigned to the closest centroid. Thus we have K collections of points call clusters. The next step is to recalculate the centroids of each cluster by taking into account the new data points added. We repeat the assignment and centroids calculation until the centroids remain the same. It can be written as follows : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Initialize cluster centers $\\mu_j$\n",
    "1. Assign an observation to the cluster center : $z_i \\leftarrow arg \\underset{j}{min} ||\\mu_j - x_i ||² $ \n",
    "2. Revise cluster centers : $\\mu_j = \\frac{1}{n_j} \\underset{i:z_i=j}\\sum x_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform k-means, we want observations within the same cluster to be very similar. Formally that means that we want to minimize the within-cluster sum of squares. As the total sum square is a constant value, by considering the Huygens theorem, we can also define the K-means algorithm as the problem of maximization of the between clusters sum of squares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like every unsupervised machine learning algorithm we have not the ground truth to evaluate the model’s performance. We don't have a reliable evaluation metric that allow as to analysze he outpout of our K-means. k-means algorithm require as inpput the the number K of cluster we want to obatain. Their is no right answer for K. Most of the time we run the algorithm with different K. For each value of K we can observerd the spread out of he clusters by using Principal conponent analysis to observed our data points in a two dimensinnal space.below we present some empirical criteria that can help us to make the choice of K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow method is an empiric criterium which help us to choose a number of cluster K. for each K, we calculate the sum of squared distance (SSE) between data points and their assigned clusters's centroids and then we plot the SSE We pick k at the spot where SSE starts to flatten out and to form an elbow as you can see on the chart below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "sse = []\n",
    "X, y = sklearn.datasets.make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Silhouette Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette measurement helps us to select the optimal number of clusters. It's helps us to determine the degree of separaions  betwenn clusters.\n",
    "\n",
    "For each sample:\n",
    "\n",
    "calculate the average distance from all data points in the same cluster ${a}_{i}$\n",
    "\n",
    "Calculate the average distance from all data points in the closest cluster ${b}_{i}$\n",
    "\n",
    "calculate the coefficient:\n",
    "$${s}_{i}= \\frac{{a}_{i} - {b}_{i}} {max\\left({a}_{i} - {b}_{i}\\right)} \\space, \\space {s}_{i} \\in [\\![-1;1]\\!] $$\n",
    "${s}_{i}= 0$ means that the sample is very close to the neighboring clusters\n",
    "\n",
    "${s}_{i}= 1$ means that the sample is far away from the neighboring clusters\n",
    "\n",
    "${s}_{i}= -1$ meas that the sample is assigned to the wrong clusters\n",
    "\n",
    "on the graphic representations below you can observe the different values of ${s}_{i}$ when you execute the kmeans on the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std= X\n",
    "for i, k in enumerate([2, 3, 4]):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    # Run the Kmeans algorithm\n",
    "    km = KMeans(n_clusters=k)\n",
    "    labels = km.fit_predict(X_std)\n",
    "    centroids = km.cluster_centers_\n",
    "\n",
    "    # Get silhouette samples\n",
    "    silhouette_vals = sklearn.metrics.silhouette_samples(X, labels)\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_ticks = []\n",
    "    y_lower, y_upper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n",
    "        cluster_silhouette_vals.sort()\n",
    "        y_upper += len(cluster_silhouette_vals)\n",
    "        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n",
    "        ax1.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n",
    "        y_lower += len(cluster_silhouette_vals)\n",
    "\n",
    "    # Get the average silhouette score and plot it\n",
    "    avg_score = np.mean(silhouette_vals)\n",
    "    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_xlabel('Silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster labels')\n",
    "    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n",
    "    \n",
    "    # Scatter plot of data colored with labels\n",
    "    ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels)\n",
    "    ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n",
    "    ax2.set_xlim([-2, 2])\n",
    "    ax2.set_xlim([-2, 2])\n",
    "    ax2.set_xlabel('Eruption time in mins')\n",
    "    ax2.set_ylabel('Waiting time to next eruption')\n",
    "    ax2.set_title('Visualization of clustered data', y=1.02)\n",
    "    ax2.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Silhouette analysis using k = {k}',\n",
    "                 fontsize=16, fontweight='semibold', y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want ${s}_{i}$ to be as close as possible to 1. So with the graphic above we have the greatest ${s}_{i}$ for K=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   $R-squares$ measure $\\left({R}^{2}\\right)$\n",
    "It is common to calculate the ratio of Between Clusters Sum of Squares and total Sum of Squares to judge the quality of the k-means outpout. We denote this ratio by ${R}^{2}$. \n",
    "\n",
    "$${R}^{2} = \\frac{Between \\space Clusters \\space Sum \\space Squares} {Total \\space Sum \\space Squares }$$\n",
    "We want the ${R}^{2}$ to be close to 1 as much as possible. As elbow method; we compute the ${R}^{2}$ for different value of K number of clusters and we make a plot with the computed ${R}^{2}$ values as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means works very well when it comes to capturing the structure of the data if clusters have a spherical shape but does an abysmal job when the clusters have complicated geometric shapes. The number K of clusters is an input for the K-means algorithm, but there is not a reliable evaluation metric.  Different initial centroids give us different final clusters. K-means does not guarantee to find the global optimum solution for clustering. The algorithm can be susceptible to outliers and noisy data: the quality of the final clustering can be highly dependent on the position of the initial cluster centroids. In other words, k-means will regularly discover a local rather than the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paragraph, we describe the basic ideas behind the latent Dirichlet allocation (LDA), which is the most straightforward topic model. The intuition behind LDA is that documents exhibit multiple topics. We formally define a topic to be a distribution over a fixed vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical preliminary to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random variables ${X}_{1},...,{X}_{n}$ are exchangeable if the n! permutations $\\left({X}_{{k}_{1}},...,{X}_{{k}_{n}}\\right)$ have the same n-dimensionnal probability distribution. The variable of an infinite sequence $\\left({X}_{n}| n \\in \\mathbb{N}^{*}\\right)$ are exchangeable if ${X}_{1},...,{X}_{m}$ are exchangeable for each $m < n $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De Finetti's theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\left({X}_{n}| n \\in \\mathbb{N}^{*}\\right)$ is an infinite sequence, ${Z}_{n}$ is a random variable. Given ${Z}_{n}$, the random variables ${X}_{n}$ are independant and have the same probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dirichlet distribution\n",
    "The Dirichlet distribution of order K ≥ 2 with parameters ${\\alpha}_{1}$, ..., ${\\alpha}_{k}$ > 0 has a probability density function with respect to Lebesgue measure on the Euclidean space $\\mathbb{R}^{K−1}$ given by : \n",
    "$$f\\left({x}_{1},...,{x}_{K},{\\alpha}_{1},...,{\\alpha}_{K}\\right)=\\frac{1}{B\\left(\\alpha\\right)}\\prod _{i=1}^{K}{{x}_{i}}^{{\\alpha}_{i}-1}$$\n",
    "where ${\\left\\{{x}_{K}\\right\\}}_{K=1}^{K=K}$  belong to the standard K − 1 simplex or in oder words $\\sum_{i=1}^{K} {x}_{i} = 1 $ and ${x}_{i} \\ge 0$ for all $ i \\in [\\![1;K]\\!]$.\\\\\n",
    "The normalizing constant is the multivariate beta function, which can be expressed in terms of the gamma function: \n",
    "$$ B\\left(\\alpha\\right)= \\frac{\\prod_{i=1}^{K}{\\Gamma\\left({\\alpha}_{i}\\right)}}{\\Gamma\\left(\\sum_{i=1}^{K} {\\alpha}_{i}\\right)};    \\alpha=\\left({\\alpha}_{1},{\\alpha}_{2}, ..., {\\alpha}_{K}\\right) $$\n",
    "$\\Gamma$ stand for the gamma function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does LDA work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Latent Dirichlet allocation LDA model is a particularly popular generative model that was introduced in 2003 by three American researchers. LDA is a generative probabilistic model. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over fixed set of words.\n",
    "Let denote by $C=\\left({d}_{1},{d}_{2},...,{d}_{m}\\right)$ our corpus.  A corpus is a collection of documents. Each document containts ${N}_{i}$ words $\\left({w}_{1},{w}_{2},...,{w}_{{N}_{i}}\\right)$. In the LDA model, we view our document as a list of words without any order, like a bag of words. so our collection of words $\\left({w}_{1},{w}_{2},...,{w}_{{N}_{i}}\\right)$ is exchangeable. De Finetti theorem ensures that, given a random variable ${Z}_{n}$, the variable ${W}_{i}$ that generate words ${w}_{i}$ are independent and have the same probability distribution.We will introduce ${Z}_{n}$ letter. The LDA is a hierarchical model at 3 levels.\n",
    "\n",
    "LDA assumes the following generative process for each document in our corpus:\n",
    "1. Choose ${N}_{i}\\sim Poisson\\left(\\xi\\right)$\n",
    "2. Choose $\\theta \\sim Dir(\\alpha)$\n",
    "3. For each ${N}_{i}$ words ${w}_{n}$ of the document $D$ :\n",
    "\n",
    "    a. Choose ${Z}_{n} \\sim Multinomial\\left(\\theta\\right)$\n",
    "    \n",
    "    b. Choose a word ${w}_{n}$ from $p\\left({w}_{n}/ {Z}_{n},\\beta\\right)$, a multinomial probabilty conditioned on the topic ${Z}_{n}$\n",
    "    \n",
    "The joint probabzlity of LDA is:\n",
    "$$p\\left(\\theta,Z,w| \\alpha, \\beta \\right)= p\\left( \\theta| \\alpha\\right)\\times \\prod_{i=1}^{N} p\\left({z}_{n}| \\theta\\right)\\times p\\left({w}_{n} | {z}_{n},\\beta\\right)$$\n",
    "However, this expression is impossible to calculate directly and accurately. It is therefore necessary to use a technique of approximation, there are two major techniques, the technique of variational inference and the technique of Gibbs Sampling which is based on the chains of Markov\n",
    "#### The previous generative scheme thus shows five parameters, as follows:\n",
    "$\\xi :$ is a random variable which gives as us the number of words in each document.  The hypothesis of Poisson distribution for ${N}_{i}$ the size of the document is not fundamental for the model. So we will assume that ${N}_{i}$ is a deterministic variable.\n",
    "\n",
    "$\\alpha :$ is a vector of dimension K (number of topic). $\\alpha$ iis the parameter of the Dirichlet distribution which generates words in our document\n",
    "\n",
    "$\\beta :$ A $K \\times V$ matrix where K is the number of topic and V the size of the vocabulary of our corpus.\n",
    "$${\\beta}_{i,j}= p\\left({w}_{n}/ {Z}_{i}\\right)$$ where ${\\beta}_{i,j}$ refers to an element of the the matrix $\\beta$. ${\\beta}_{i,j}$ is the probability of a word ${w}_{i}$ to belong to a topic. The estimation of $\\beta$ has a critical importance in the LDA  model. an inference is made on this parameter.\n",
    "\n",
    "$\\theta$ et $Z :$ are latent parameters of the model, hence the name LDA.\n",
    "\n",
    "$\\theta$ is the exact portion of the topics in the document as $\\alpha$ gives the mean proportion.\n",
    "\n",
    "$Z :{Z}_{i} \\in \\left\\{1,...,{N}_{i}\\right\\}$ is the topic associates with every word ${w}_{i}$ of our document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1] Christopher Manning, *Lecture 2 : Word Vector Representations: word2vec*, Stanford University, delivered 3 April 2017 on Youtube.\n",
    "- Tomas Mikolov, et al.: *Efficient Estimation of Word Representations in Vector Space*, 2013.\n",
    "- Tomas Mikolov, et al.: *Distributed Representations of Words and Phrases and their Compositionality*, NIPS 2013.\n",
    "- Gigaword \n",
    "\n",
    "https://catalog.ldc.upenn.edu/LDC2011T07 \n",
    "https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/cluster-k-means/interpret-the-results/all-statistics-and-graphs\n",
    "\n",
    "https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce\n",
    "\n",
    "\n",
    "https://medium.com/@adriensieg/text-similarities-da019229c894\n",
    "\n",
    "\n",
    "https://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/\n",
    "\n",
    "https://medium.com/@adriensieg/text-similarities-da019229c894\n",
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "https://towardsdatascience.com/k-means-clustering-8e1e64c1561c\n",
    "https://ai.intelligentonlinetools.com/ml/k-means-clustering-example-word2vec/\n",
    "\n",
    "https://rlbarter.github.io/superheat-examples/word2vec/\n",
    "\n",
    "https://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/\n",
    "\n",
    "\n",
    "https://www-users.cs.umn.edu/~kumar001/dmbook/ch8.pdf\n",
    "\n",
    "https://www.researchgate.net/publication/300699735_The_Clustering_Validity_with_Silhouette_and_Sum_of_Squared_Errors\n",
    "\n",
    "https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
    "\n",
    "https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim (pour Latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@inproceedings{rehurek_lrec,\n",
    "      title = {{Software Framework for Topic Modelling with Large Corpora}},\n",
    "      author = {Radim {\\v R}eh{\\r u}{\\v r}ek and Petr Sojka},\n",
    "      booktitle = {{Proceedings of the LREC 2010 Workshop on New\n",
    "           Challenges for NLP Frameworks}},\n",
    "      pages = {45--50},\n",
    "      year = 2010,\n",
    "      month = May,\n",
    "      day = 22,\n",
    "      publisher = {ELRA},\n",
    "      address = {Valletta, Malta},\n",
    "      note={\\url{http://is.muni.cz/publication/884893/en}},\n",
    "      language={English}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
